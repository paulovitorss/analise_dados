{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Importe as bibliotecas necessárias",
   "id": "bc14dafce2caf907"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "'''\n",
    "# Instalando as bibliotecas necessárias\n",
    "%pip install -U pip setuptools wheel\n",
    "%pip install pymongo\n",
    "%pip install pandas\n",
    "%pip install nltk\n",
    "%pip install wordcloud\n",
    "%pip install spacy\n",
    "%pip install matplotlib\n",
    "%pip install numpy==1.26.4\n",
    "%pip install -U scikit-learn\n",
    "%pip install unidecode\n",
    "'''"
   ],
   "id": "7f2d74b00c76fd40",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Importando as bibliotecas",
   "id": "2bf9d9363b7b99f2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "nltk.download('omw-1.4', quiet=True)\n",
    "nltk.download('averaged_perceptron_tagger', quiet=True)\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('punkt_tab', quiet=True)\n",
    "nltk.download('averaged_perceptron_tagger_eng', quiet=True)\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from db import connection_db as conndb\n",
    "from db import filters\n",
    "from utils import plot_graphs\n",
    "from utils.text_treatment import TextTreatment\n",
    "from dateutil.relativedelta import relativedelta"
   ],
   "id": "4761db26da7a2327",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "mongo_connection = conndb.MongoDBConnection(uri='mongodb://localhost:27017/', database_name='dadosVivamente',\n",
    "                                            collection_name='dadosSemFiltros')\n",
    "mongo_connection.connect()\n",
    "collection = mongo_connection.collection"
   ],
   "id": "7a6bb32be9aeb75a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "collection_filters = filters.CollectionFilters(collection)\n",
    "collection_filters.apply_pipeline1('dadosComFiltrosIniciais')\n",
    "collection_filters.apply_pipeline2(7, 2, 'posts7anos2anos')\n",
    "collection_filters.apply_pipeline3('postsComBDIAndInfos')\n",
    "collection_filters.apply_pipeline4('postsComBDIAndInfosFiltroDataPosts')\n",
    "data_inicio = datetime(2017, 12, 1)\n",
    "data_fim = data_inicio - relativedelta(months=6)\n",
    "collection_filters.apply_pipeline5('postsFiltradosPorData', data_inicio, data_fim)\n",
    "collection_filters.quant_users_cat('suicida', '$eq', '3')\n",
    "collection_filters.count_users_by_gender('suicida', '$eq', '3', 'M')\n",
    "collection_filters.count_users_by_gender('suicida', '$eq', '3', 'F')\n",
    "collection = collection_filters.collection"
   ],
   "id": "ba5a47bd08e116ad",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Filtrando documentos com o atributo maior que 3\n",
    "filtro = {\"suicida\": {\"$eq\": \"3\"}}\n",
    "documentos = collection.find(filtro)"
   ],
   "id": "ea19f91952d67bca",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Transformando os documentos em um DataFrame\n",
    "df = pd.DataFrame(list(documentos))\n",
    "df.head()"
   ],
   "id": "8e80f1ac50e27240",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(df.dtypes)\n",
    "# Lista de colunas que precisam ser convertidas\n",
    "colunas_para_converter = [\n",
    "    'pessimismo', 'tristeza', 'fracasso', 'prazer', 'culpa', 'punicao', 'estima',\n",
    "    'critica', 'suicida', 'choro', 'agitacao', 'interesse', 'indecisao',\n",
    "    'desvalorizacao', 'energia', 'sono', 'irritabilidade', 'apetite',\n",
    "    'concentracao', 'fadiga', 'int_sexo', 'quantAmigos'\n",
    "]\n",
    "\n",
    "df[colunas_para_converter] = df[colunas_para_converter].astype('int64')"
   ],
   "id": "a6cd10d242776211",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Salvar o DataFrame em um arquivo CSV\n",
    "df.to_csv('dados/com_filtros_datas/6meses/so_suicida_6_meses.csv', index=False)"
   ],
   "id": "dffd3dec7a6ba792",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Agrupar por usuário, mês e ano\n",
    "posts_grouped = df.groupby(['id_usuario', 'mes', 'ano']).size().reset_index(name='quantidade')\n",
    "\n",
    "# Adicionar coluna com o período e converter para datetime\n",
    "posts_grouped['periodo'] = pd.to_datetime(posts_grouped['mes'].astype(str) + '/' + posts_grouped['ano'].astype(str),\n",
    "                                          format='%m/%Y')\n",
    "\n",
    "# Deve retornar 0 se a conversão foi bem-sucedida.\n",
    "print(posts_grouped['periodo'].isnull().sum())\n",
    "\n",
    "posts_grouped.dtypes"
   ],
   "id": "337f2d80271498b7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Plotar quantidade de posts por usuário",
   "id": "b309a737c28177ea"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "plot_graphs.PlotGraphs().plot_posts_per_user(posts_grouped, 'dados/com_filtros_datas/6meses/graficos')",
   "id": "723527524f4964ec",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Efentuando a limpeza dos dados",
   "id": "44e73a070a35b8a2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df['postMessageLimpo'] = df['postMessage'].fillna('').apply(lambda text: TextTreatment(text).preprocessamento_texto())",
   "id": "1f4ea101d719870e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Exibir apenas as colunas postMessage e postMessageLimpo\n",
    "df[['postMessage', 'postMessageLimpo']].head()"
   ],
   "id": "61dbc3fbbde3d1f9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# def get_stopwords() -> list:\n",
    "#     portuguese_ingles_stopwords = []\n",
    "# \n",
    "#     # Stopwords em português\n",
    "#     portugues = [unidecode.unidecode(palavra.lower().replace(\" \", \"\")) for palavra in\n",
    "#                  nltk.corpus.stopwords.words('portuguese')]\n",
    "#     portuguese_ingles_stopwords.extend(portugues)\n",
    "# \n",
    "#     # Stopwords em inglês\n",
    "#     ingles = [unidecode.unidecode(palavra.lower().replace(\" \", \"\")) for palavra in\n",
    "#               nltk.corpus.stopwords.words('english')]\n",
    "#     portuguese_ingles_stopwords.extend(ingles)\n",
    "# \n",
    "#     # Stopwords de diferentes fontes\n",
    "#     stopwords_wordcloud = [unidecode.unidecode(palavra.lower().replace(\" \", \"\")) for palavra in STOPWORDS]\n",
    "#     portuguese_ingles_stopwords.extend(stopwords_wordcloud)\n",
    "# \n",
    "#     stopwords_sklearn = [unidecode.unidecode(word.lower().replace(\" \", \"\")) for word in STOP_WORDS]\n",
    "#     portuguese_ingles_stopwords.extend(stopwords_sklearn)\n",
    "# \n",
    "#     # Adicionar stopwords do arquivo customizado\n",
    "#     with open('dados/datasets/stopwords-pt.txt', 'r', encoding='utf-8') as words:\n",
    "#         custom_stopwords = [unidecode.unidecode(word.lower().strip()) for word in words if word.strip()]\n",
    "#     portuguese_ingles_stopwords.extend(custom_stopwords)\n",
    "# \n",
    "#     # Adicionar nomes\n",
    "#     with open('dados/datasets/nomes.txt', 'r', encoding='utf-8') as words:\n",
    "#         nomes_stopwords = [unidecode.unidecode(word.lower().strip()) for word in words if word.strip()]\n",
    "#     portuguese_ingles_stopwords.extend(nomes_stopwords)\n",
    "# \n",
    "#     # Remover duplicatas e ordenar a lista\n",
    "#     portuguese_ingles_stopwords = list(set(portuguese_ingles_stopwords))\n",
    "#     portuguese_ingles_stopwords.sort()\n",
    "# \n",
    "#     return portuguese_ingles_stopwords\n",
    "# \n",
    "# \n",
    "# def remocao_stopword(string, lista_stopwords) -> str:\n",
    "#     a = [i for i in string.split() if i not in lista_stopwords]\n",
    "#     return ' '.join(a)\n",
    "# \n",
    "# \n",
    "# def remove_caracteres(text) -> str:\n",
    "#     text = text.lower().strip()\n",
    "#     text = re.compile(r'<.*?>').sub('', text)\n",
    "#     text = re.compile(r'[%s]' % re.escape(string.punctuation)).sub(' ', text)\n",
    "#     text = re.sub(r'\\s+', ' ', text)\n",
    "#     text = re.sub(r'\\[[0-9]*\\]', ' ', text)\n",
    "#     text = re.sub(r'[^\\w\\s]', '', str(text).lower().strip())\n",
    "#     text = re.sub(r'\\d', ' ', text)\n",
    "#     text = re.sub(r\"\\$\", \"\", text)\n",
    "#     text = re.sub(r\"https?:\\/\\/.*[\\r\\n]*\", \"\", text)\n",
    "#     text = re.sub(r\"#\", \"\", text)\n",
    "#     text = re.sub(r'https?:\\/\\/[\\r\\n],\"[\\r\\n]\"', '', text, flags=re.MULTILINE)\n",
    "#     text = re.sub(r'\\<a href', ' ', text)\n",
    "#     text = re.sub(r'&amp;', '', text)\n",
    "#     text = re.sub(r'[_\"\\-;%()|+&=*%.,!?:#$@\\[\\]/]', ' ', text)\n",
    "#     text = re.sub(r'<br />', ' ', text)\n",
    "#     text = re.sub(r'\\'', ' ', text)\n",
    "#     text = re.sub(r'[^a-zà-ù ]', ' ', text)\n",
    "#     text = re.sub(r'k{2,}', '', text, flags=re.IGNORECASE)\n",
    "#     text = re.sub(r'j{2,}', '', text, flags=re.IGNORECASE)\n",
    "#     text = re.compile(r\"[\"\n",
    "#                       u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "#                       u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "#                       u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "#                       u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "#                       u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
    "#                       u\"\\U00002702-\\U000027B0\"\n",
    "#                       u\"\\U00002702-\\U000027B0\"\n",
    "#                       u\"\\U000024C2-\\U0001F251\"\n",
    "#                       u\"\\U0001f926-\\U0001f937\"\n",
    "#                       u\"\\U00010000-\\U0010ffff\"\n",
    "#                       u\"\\u2640-\\u2642\"\n",
    "#                       u\"\\u2600-\\u2B55\"\n",
    "#                       u\"\\u200d\"\n",
    "#                       u\"\\u23cf\"\n",
    "#                       u\"\\u23e9\"\n",
    "#                       u\"\\u231a\"\n",
    "#                       u\"\\ufe0f\"  # dingbats\n",
    "#                       u\"\\u3030\"\n",
    "#                       \"]+\", flags=re.UNICODE).sub(r'', text)\n",
    "#     return text\n",
    "# \n",
    "# \n",
    "# def obter_pos_tag(token) -> str:\n",
    "#     if token.startswith('J'):\n",
    "#         return wordnet.ADJ\n",
    "#     elif token.startswith('V'):\n",
    "#         return wordnet.VERB\n",
    "#     elif token.startswith('N'):\n",
    "#         return wordnet.NOUN\n",
    "#     elif token.startswith('R'):\n",
    "#         return wordnet.ADV\n",
    "#     else:\n",
    "#         return wordnet.NOUN\n",
    "# \n",
    "# \n",
    "# def lematizacao(string) -> str:\n",
    "#     token = word_tokenize(string)\n",
    "#     word_pos_tags = nltk.pos_tag(token)\n",
    "#     wl = WordNetLemmatizer()\n",
    "#     a = [wl.lemmatize(tag[0], obter_pos_tag(tag[1])) for idx, tag in\n",
    "#          enumerate(word_pos_tags)]\n",
    "#     return \" \".join(a)\n",
    "# \n",
    "# \n",
    "# def preprocessamento_texto(texto_limpo) -> str:\n",
    "#     lista_stopwords = get_stopwords()\n",
    "#     texto_limpo = remove_caracteres(texto_limpo)\n",
    "#     texto_limpo = remocao_stopword(texto_limpo, lista_stopwords)\n",
    "#     texto_limpo = lematizacao(texto_limpo)\n",
    "#     return texto_limpo"
   ],
   "id": "aceb4344b9b49f99",
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "corpus = df['postMessageLimpo'].tolist()",
   "id": "55003b9f9e6eaac5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "df.head()"
   ],
   "id": "77e1f726987e810",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Carregar stopwords em português para os TF-IDF e Bag of Words\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = stopwords.words('portuguese')"
   ],
   "id": "fc92b46e4febeaf9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Criar o vetorizador TF-IDF com parâmetros ajustados\n",
    "vectorizer = TfidfVectorizer(\n",
    "    stop_words=stop_words,\n",
    "    max_features=None,\n",
    "    max_df=0.40,\n",
    "    min_df=5,\n",
    "    ngram_range=(1, 1),\n",
    "    sublinear_tf=True\n",
    ")\n",
    "\n",
    "corpus_tfidf = df['postMessageLimpo'].tolist()\n",
    "tfidf_matrix = vectorizer.fit_transform(corpus_tfidf)\n",
    "\n",
    "# Obter as palavras\n",
    "palavras_tfidf = vectorizer.get_feature_names_out()\n",
    "# Lista para armazenar os resultados\n",
    "resultados_tfidf = []\n",
    "\n",
    "# Iterar sobre cada usuário\n",
    "for usuario in df['id_usuario'].unique():\n",
    "    # Filtrar os textos do usuário\n",
    "    indices_usuario = df[df['id_usuario'] == usuario].index\n",
    "    if len(indices_usuario) > 0:\n",
    "        # Extrair as palavras com maior score TF-IDF para o usuário\n",
    "        user_tfidf = tfidf_matrix[indices_usuario]\n",
    "        user_tfidf_mean = np.asarray(user_tfidf.mean(axis=0)).flatten()\n",
    "\n",
    "        # Obter os índices das 10 palavras com maior score\n",
    "        top_10_indices = user_tfidf_mean.argsort()[-10:][::-1]\n",
    "\n",
    "        # Adicionar as palavras e seus scores à lista de resultados\n",
    "        for index in top_10_indices:\n",
    "            resultados_tfidf.append({\n",
    "                'id_usuario': usuario,\n",
    "                'palavra': palavras_tfidf[index],\n",
    "                'score': user_tfidf_mean[index]\n",
    "            })\n",
    "\n",
    "# Converter a lista de resultados em DataFrame\n",
    "resultados_df_tfidf = pd.DataFrame(resultados_tfidf)\n",
    "resultados_df_tfidf.to_csv('dados/com_filtros_datas/6meses/so_suicida_resultados_tfidf_unigramas.csv', index=False)\n",
    "\n",
    "print(resultados_df_tfidf)"
   ],
   "id": "2f6d54daf418e3e4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Usando o algoritmo Bag of Words",
   "id": "c6ce4c689fd09eab"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Criar o vetorizador Bag of Words com parâmetros ajustados\n",
    "vectorizer = CountVectorizer(\n",
    "    stop_words=stop_words,\n",
    "    ngram_range=(1, 1),\n",
    "    max_df=0.85,\n",
    "    min_df=5\n",
    ")\n",
    "\n",
    "corpus_bow = df['postMessageLimpo'].tolist()\n",
    "bow_matrix = vectorizer.fit_transform(corpus_bow)\n",
    "\n",
    "palavras_bow = vectorizer.get_feature_names_out()\n",
    "\n",
    "resultados_bow = []\n",
    "\n",
    "for usuario in df['id_usuario'].unique():\n",
    "    indices_usuario = df[df['id_usuario'] == usuario].index\n",
    "    if len(indices_usuario) > 0:\n",
    "        user_bow = bow_matrix[indices_usuario]\n",
    "        user_bow_sum = np.asarray(user_bow.sum(axis=0)).flatten()\n",
    "\n",
    "        top_10_indices_bow = user_bow_sum.argsort()[-10:][::-1]\n",
    "\n",
    "        for index in top_10_indices_bow:\n",
    "            resultados_bow.append({\n",
    "                'id_usuario': usuario,\n",
    "                'palavra': palavras_bow[index],\n",
    "                'contagem': user_bow_sum[index]\n",
    "            })\n",
    "\n",
    "# Converter a lista de resultados em DataFrame\n",
    "resultados_df_bow = pd.DataFrame(resultados_bow)\n",
    "resultados_df_bow.to_csv('dados/com_filtros_datas/6meses/so_suicida_resultados_bow_unigramas.csv', index=False)\n",
    "\n",
    "print(resultados_df_bow)"
   ],
   "id": "2a1dc735f1d6e111",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
