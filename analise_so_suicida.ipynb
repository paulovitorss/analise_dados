{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Importe as bibliotecas necessárias",
   "id": "bc14dafce2caf907"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-06T15:33:31.227901Z",
     "start_time": "2024-09-06T15:33:31.223437Z"
    }
   },
   "cell_type": "code",
   "source": [
    "'''\n",
    "# Instalando as bibliotecas necessárias\n",
    "%pip install -U pip setuptools wheel\n",
    "%pip install pymongo\n",
    "%pip install pandas\n",
    "%pip install nltk\n",
    "%pip install wordcloud\n",
    "%pip install spacy\n",
    "%pip install matplotlib\n",
    "%pip install numpy==1.26.4\n",
    "%pip install -U scikit-learn\n",
    "%pip install unidecode\n",
    "'''"
   ],
   "id": "7f2d74b00c76fd40",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Instalando as bibliotecas necessárias\\n%pip install -U pip setuptools wheel\\n%pip install pymongo\\n%pip install pandas\\n%pip install nltk\\n%pip install wordcloud\\n%pip install spacy\\n%pip install matplotlib\\n%pip install numpy==1.26.4\\n%pip install -U scikit-learn\\n%pip install unidecode\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-06T15:33:32.778685Z",
     "start_time": "2024-09-06T15:33:31.271870Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Importando as bibliotecas\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "nltk.download('omw-1.4', quiet=True)\n",
    "nltk.download('averaged_perceptron_tagger', quiet=True)\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('punkt_tab', quiet=True)\n",
    "nltk.download('averaged_perceptron_tagger_eng', quiet=True)\n",
    "from wordcloud.wordcloud import STOPWORDS\n",
    "from spacy.lang.pt.stop_words import STOP_WORDS\n",
    "import re\n",
    "import string\n",
    "from datetime import datetime, timedelta\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import unidecode\n",
    "from db import connection_db as conndb\n",
    "from db import filters\n",
    "from dateutil.relativedelta import relativedelta"
   ],
   "id": "4761db26da7a2327",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-06T15:33:34.718188Z",
     "start_time": "2024-09-06T15:33:34.711611Z"
    }
   },
   "cell_type": "code",
   "source": [
    "mongo_connection = conndb.MongoDBConnection(uri='mongodb://localhost:27017/', database_name='dadosVivamente',\n",
    "                                            collection_name='dadosSemFiltros')\n",
    "mongo_connection.connect()\n",
    "db = mongo_connection.db\n",
    "collection = db['dadosSemFiltros']"
   ],
   "id": "7a6bb32be9aeb75a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conexão estabelecida com sucesso ao banco de dados.\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-06T15:33:35.941634Z",
     "start_time": "2024-09-06T15:33:34.817030Z"
    }
   },
   "cell_type": "code",
   "source": [
    "collection_filters = filters.CollectionFilters(collection)\n",
    "collection_filters.apply_pipeline1('dadosComFiltrosIniciais')\n",
    "collection_filters.apply_pipeline2(7, 2, 'posts7anos2anos')\n",
    "collection_filters.apply_pipeline3('postsComBDIAndInfos')\n",
    "collection_filters.apply_pipeline4('postsComBDIAndInfosFiltroDataPosts')\n",
    "data_inicio = datetime(2017, 12, 1)\n",
    "data_fim = data_inicio - relativedelta(months=6)\n",
    "collection_filters.apply_pipeline5('postsFiltradosPorData', data_inicio, data_fim)\n",
    "collection_filters.quant_users_cat('suicida', '$eq', '3')\n",
    "collection_filters.count_users_by_gender('suicida', '$eq', '3', 'M')\n",
    "collection_filters.count_users_by_gender('suicida', '$eq', '3', 'F')"
   ],
   "id": "ba5a47bd08e116ad",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:A coleção já existe: dadosComFiltrosIniciais\n",
      "INFO:root:A coleção já existe: posts7anos2anos\n",
      "INFO:root:A coleção já existe: postsComBDIAndInfos\n",
      "INFO:root:A coleção já existe: postsComBDIAndInfosFiltroDataPosts\n",
      "INFO:root:Criando a coleção: postsFiltradosPorData\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantidade de Usuários: 58\n",
      "Quantidade de usuários M: 11\n",
      "Quantidade de usuários F: 47\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Filtro de 6 meses antes do mês da coleta",
   "id": "65a59d6b2a783f80"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "if 'posts6Meses' in db.list_collection_names():\n",
    "    print('A coleção já existe')\n",
    "    collection = db['posts6Meses']\n",
    "else:\n",
    "    data_inicio = datetime(2017, 12, 1)\n",
    "\n",
    "    data_fim = data_inicio - timedelta(days=6 * 30)\n",
    "\n",
    "    pipeline8 = [\n",
    "        {\n",
    "            '$match': {\n",
    "                'postCreated_time': {\n",
    "                    '$gte': data_fim,\n",
    "                    '$lt': data_inicio\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            '$out': 'posts6Meses'\n",
    "        }\n",
    "    ]\n",
    "    collection.aggregate(pipeline8)\n",
    "    collection = db['posts6Meses']"
   ],
   "id": "c4f207b8e88f2915",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Filtrando documentos com o atributo maior que 3\n",
    "filtro = {\"suicida\": {\"$eq\": \"3\"}}\n",
    "documentos = collection.find(filtro)"
   ],
   "id": "ea19f91952d67bca",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Transformando os documentos em um DataFrame\n",
    "df = pd.DataFrame(list(documentos))"
   ],
   "id": "8e80f1ac50e27240",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df['data'] = pd.to_datetime(df['postCreated_time'])\n",
    "df.head()"
   ],
   "id": "e5d547867db365ac",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "print(df.dtypes)",
   "id": "7cb7d55ccefa2446",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Lista de colunas que precisam ser convertidas\n",
    "colunas_para_converter = [\n",
    "    'pessimismo', 'tristeza', 'fracasso', 'prazer', 'culpa', 'punicao', 'estima',\n",
    "    'critica', 'suicida', 'choro', 'agitacao', 'interesse', 'indecisao',\n",
    "    'desvalorizacao', 'energia', 'sono', 'irritabilidade', 'apetite',\n",
    "    'concentracao', 'fadiga', 'int_sexo', 'quantAmigos'\n",
    "]\n",
    "\n",
    "df[colunas_para_converter] = df[colunas_para_converter].astype('int64')"
   ],
   "id": "a6cd10d242776211",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Salvar o DataFrame em um arquivo CSV\n",
    "df.to_csv('dados/com_filtros_datas/6meses/so_suicida_6_meses.csv', index=False)"
   ],
   "id": "dffd3dec7a6ba792",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# filtrar pelo id_usuario\n",
    "# df = df[df['id_usuario'] == '1022864967872047']"
   ],
   "id": "b0e23ed150a7110",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Agrupar por usuário, mês e ano\n",
    "posts_grouped = df.groupby(['id_usuario', 'mes', 'ano']).size().reset_index(name='quantidade')\n",
    "\n",
    "# Adicionar coluna com o período\n",
    "posts_grouped['periodo'] = posts_grouped['mes'].astype(str) + '/' + posts_grouped['ano'].astype(str)"
   ],
   "id": "337f2d80271498b7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Plotar quantidade de posts por usuário",
   "id": "b309a737c28177ea"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# # plotar quantidade de posts por usuario\n",
    "# \n",
    "# # Criar o gráfico de linha para cada usuário\n",
    "# for usuario in posts_grouped['id_usuario'].unique():\n",
    "#     df_usuario = posts_grouped[posts_grouped['id_usuario'] == usuario].copy()\n",
    "# \n",
    "#     # Convertendo a coluna 'periodo' para datetime para garantir a ordenação correta\n",
    "#     df_usuario['periodo'] = pd.to_datetime(df_usuario['periodo'], format='%m/%Y', errors='coerce')\n",
    "# \n",
    "#     # Ordenar os dados por 'periodo'\n",
    "#     df_usuario = df_usuario.sort_values('periodo')\n",
    "# \n",
    "#     # Configurar o gráfico de linha\n",
    "#     plt.figure(figsize=(20, 8))  # Aumentar o tamanho da figura\n",
    "# \n",
    "#     plt.plot(df_usuario['periodo'].dt.strftime('%m/%Y'), df_usuario['quantidade'], marker='o', linestyle='-',\n",
    "#              color='blue')\n",
    "# \n",
    "#     # Adicionar título e rótulos\n",
    "#     plt.title(f'Quantidade de Posts por Mês/Ano - Usuário: {usuario}')\n",
    "#     plt.xlabel('Mês/Ano')\n",
    "#     plt.ylabel('Quantidade de Posts')\n",
    "# \n",
    "#     # Melhorar a legibilidade dos rótulos do eixo X\n",
    "#     plt.xticks(rotation=45, ha='right', fontsize=10)\n",
    "# \n",
    "#     # Aumentar o pad dos rótulos do eixo X\n",
    "#     plt.gca().tick_params(axis='x', pad=14)  # Aumenta o espaço entre os rótulos e o eixo\n",
    "# \n",
    "#     # Adicionar grid\n",
    "#     plt.grid(True, axis='y')\n",
    "# \n",
    "#     # Ajustar o layout para evitar sobreposição, com mais padding\n",
    "#     plt.tight_layout(pad=8.0)  # Aumentar o padding geral do layout\n",
    "# \n",
    "#     # Adicionar espaço extra ao layout se necessário\n",
    "#     plt.subplots_adjust(bottom=0.2)  # Adiciona mais espaço abaixo dos rótulos do eixo X\n",
    "# \n",
    "#     # Salvando o gráfico no diretório dados/com_filtros_datas/6meses/graficos\n",
    "#     plt.savefig(f'dados/com_filtros_datas/6meses/graficos/quantidade_posts_{usuario}.png')\n",
    "# \n",
    "#     # Mostrar o gráfico\n",
    "#     plt.show()\n",
    "# \n",
    "#     # Fechar a figura explicitamente para liberar memória\n",
    "#     plt.close()"
   ],
   "id": "99c10501106abed1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Efentuando a limpeza dos dados",
   "id": "44e73a070a35b8a2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def get_stopwords() -> list:\n",
    "    portuguese_ingles_stopwords = []\n",
    "\n",
    "    # Stopwords em português\n",
    "    portugues = [unidecode.unidecode(palavra.lower().replace(\" \", \"\")) for palavra in\n",
    "                 nltk.corpus.stopwords.words('portuguese')]\n",
    "    portuguese_ingles_stopwords.extend(portugues)\n",
    "\n",
    "    # Stopwords em inglês\n",
    "    ingles = [unidecode.unidecode(palavra.lower().replace(\" \", \"\")) for palavra in\n",
    "              nltk.corpus.stopwords.words('english')]\n",
    "    portuguese_ingles_stopwords.extend(ingles)\n",
    "\n",
    "    # Stopwords de diferentes fontes\n",
    "    stopwords_wordcloud = [unidecode.unidecode(palavra.lower().replace(\" \", \"\")) for palavra in STOPWORDS]\n",
    "    portuguese_ingles_stopwords.extend(stopwords_wordcloud)\n",
    "\n",
    "    stopwords_sklearn = [unidecode.unidecode(word.lower().replace(\" \", \"\")) for word in STOP_WORDS]\n",
    "    portuguese_ingles_stopwords.extend(stopwords_sklearn)\n",
    "\n",
    "    # Adicionar stopwords do arquivo customizado\n",
    "    with open('dados/datasets/stopwords-pt.txt', 'r', encoding='utf-8') as words:\n",
    "        custom_stopwords = [unidecode.unidecode(word.lower().strip()) for word in words if word.strip()]\n",
    "    portuguese_ingles_stopwords.extend(custom_stopwords)\n",
    "\n",
    "    # Adicionar nomes\n",
    "    with open('dados/datasets/nomes.txt', 'r', encoding='utf-8') as words:\n",
    "        nomes_stopwords = [unidecode.unidecode(word.lower().strip()) for word in words if word.strip()]\n",
    "    portuguese_ingles_stopwords.extend(nomes_stopwords)\n",
    "\n",
    "    # Remover duplicatas e ordenar a lista\n",
    "    portuguese_ingles_stopwords = list(set(portuguese_ingles_stopwords))\n",
    "    portuguese_ingles_stopwords.sort()\n",
    "\n",
    "    return portuguese_ingles_stopwords\n",
    "\n",
    "\n",
    "def remocao_stopword(string, lista_stopwords) -> str:\n",
    "    a = [i for i in string.split() if i not in lista_stopwords]\n",
    "    return ' '.join(a)\n",
    "\n",
    "\n",
    "def remove_caracteres(text) -> str:\n",
    "    text = text.lower().strip()\n",
    "    text = re.compile(r'<.*?>').sub('', text)\n",
    "    text = re.compile(r'[%s]' % re.escape(string.punctuation)).sub(' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = re.sub(r'\\[[0-9]*\\]', ' ', text)\n",
    "    text = re.sub(r'[^\\w\\s]', '', str(text).lower().strip())\n",
    "    text = re.sub(r'\\d', ' ', text)\n",
    "    text = re.sub(r\"\\$\", \"\", text)\n",
    "    text = re.sub(r\"https?:\\/\\/.*[\\r\\n]*\", \"\", text)\n",
    "    text = re.sub(r\"#\", \"\", text)\n",
    "    text = re.sub(r'https?:\\/\\/[\\r\\n],\"[\\r\\n]\"', '', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'\\<a href', ' ', text)\n",
    "    text = re.sub(r'&amp;', '', text)\n",
    "    text = re.sub(r'[_\"\\-;%()|+&=*%.,!?:#$@\\[\\]/]', ' ', text)\n",
    "    text = re.sub(r'<br />', ' ', text)\n",
    "    text = re.sub(r'\\'', ' ', text)\n",
    "    text = re.sub(r'[^a-zà-ù ]', ' ', text)\n",
    "    text = re.sub(r'k{2,}', '', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'j{2,}', '', text, flags=re.IGNORECASE)\n",
    "    text = re.compile(r\"[\"\n",
    "                      u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                      u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                      u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                      u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                      u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
    "                      u\"\\U00002702-\\U000027B0\"\n",
    "                      u\"\\U00002702-\\U000027B0\"\n",
    "                      u\"\\U000024C2-\\U0001F251\"\n",
    "                      u\"\\U0001f926-\\U0001f937\"\n",
    "                      u\"\\U00010000-\\U0010ffff\"\n",
    "                      u\"\\u2640-\\u2642\"\n",
    "                      u\"\\u2600-\\u2B55\"\n",
    "                      u\"\\u200d\"\n",
    "                      u\"\\u23cf\"\n",
    "                      u\"\\u23e9\"\n",
    "                      u\"\\u231a\"\n",
    "                      u\"\\ufe0f\"  # dingbats\n",
    "                      u\"\\u3030\"\n",
    "                      \"]+\", flags=re.UNICODE).sub(r'', text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def obter_pos_tag(token) -> str:\n",
    "    if token.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif token.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif token.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif token.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "\n",
    "\n",
    "def lematizacao(string) -> str:\n",
    "    token = word_tokenize(string)\n",
    "    word_pos_tags = nltk.pos_tag(token)\n",
    "    wl = WordNetLemmatizer()\n",
    "    a = [wl.lemmatize(tag[0], obter_pos_tag(tag[1])) for idx, tag in\n",
    "         enumerate(word_pos_tags)]\n",
    "    return \" \".join(a)\n",
    "\n",
    "\n",
    "def preprocessamento_texto(texto_limpo) -> str:\n",
    "    lista_stopwords = get_stopwords()\n",
    "    texto_limpo = remove_caracteres(texto_limpo)\n",
    "    texto_limpo = remocao_stopword(texto_limpo, lista_stopwords)\n",
    "    texto_limpo = lematizacao(texto_limpo)\n",
    "    return texto_limpo"
   ],
   "id": "aceb4344b9b49f99",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df['postMessageLimpo'] = df['postMessage'].fillna('').apply(preprocessamento_texto)\n",
    "df.head()\n",
    "corpus = df['postMessageLimpo'].tolist()"
   ],
   "id": "55003b9f9e6eaac5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "df.head()"
   ],
   "id": "77e1f726987e810",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Carregar stopwords em português para os TF-IDF e Bag of Words\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = stopwords.words('portuguese')"
   ],
   "id": "fc92b46e4febeaf9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Criar o vetorizador TF-IDF com parâmetros ajustados\n",
    "vectorizer = TfidfVectorizer(\n",
    "    stop_words=stop_words,\n",
    "    max_features=None,\n",
    "    max_df=0.40,\n",
    "    min_df=5,\n",
    "    ngram_range=(1, 1),\n",
    "    sublinear_tf=True\n",
    ")\n",
    "\n",
    "corpus_tfidf = df['postMessageLimpo'].tolist()\n",
    "tfidf_matrix = vectorizer.fit_transform(corpus_tfidf)\n",
    "\n",
    "# Obter as palavras\n",
    "palavras_tfidf = vectorizer.get_feature_names_out()\n",
    "# Lista para armazenar os resultados\n",
    "resultados_tfidf = []\n",
    "\n",
    "# Iterar sobre cada usuário\n",
    "for usuario in df['id_usuario'].unique():\n",
    "    # Filtrar os textos do usuário\n",
    "    indices_usuario = df[df['id_usuario'] == usuario].index\n",
    "    if len(indices_usuario) > 0:\n",
    "        # Extrair as palavras com maior score TF-IDF para o usuário\n",
    "        user_tfidf = tfidf_matrix[indices_usuario]\n",
    "        user_tfidf_mean = np.asarray(user_tfidf.mean(axis=0)).flatten()\n",
    "\n",
    "        # Obter os índices das 10 palavras com maior score\n",
    "        top_10_indices = user_tfidf_mean.argsort()[-10:][::-1]\n",
    "\n",
    "        # Adicionar as palavras e seus scores à lista de resultados\n",
    "        for index in top_10_indices:\n",
    "            resultados_tfidf.append({\n",
    "                'id_usuario': usuario,\n",
    "                'palavra': palavras_tfidf[index],\n",
    "                'score': user_tfidf_mean[index]\n",
    "            })\n",
    "\n",
    "# Converter a lista de resultados em DataFrame\n",
    "resultados_df_tfidf = pd.DataFrame(resultados_tfidf)\n",
    "resultados_df_tfidf.to_csv('dados/com_filtros_datas/6meses/so_suicida_resultados_tfidf_unigramas.csv', index=False)\n",
    "\n",
    "print(resultados_df_tfidf)"
   ],
   "id": "2f6d54daf418e3e4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Usando o algoritmo Bag of Words",
   "id": "c6ce4c689fd09eab"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Criar o vetorizador Bag of Words com parâmetros ajustados\n",
    "vectorizer = CountVectorizer(\n",
    "    stop_words=stop_words,\n",
    "    ngram_range=(1, 1),\n",
    "    max_df=0.85,\n",
    "    min_df=5\n",
    ")\n",
    "\n",
    "corpus_bow = df['postMessageLimpo'].tolist()\n",
    "bow_matrix = vectorizer.fit_transform(corpus_bow)\n",
    "\n",
    "palavras_bow = vectorizer.get_feature_names_out()\n",
    "\n",
    "resultados_bow = []\n",
    "\n",
    "for usuario in df['id_usuario'].unique():\n",
    "    indices_usuario = df[df['id_usuario'] == usuario].index\n",
    "    if len(indices_usuario) > 0:\n",
    "        user_bow = bow_matrix[indices_usuario]\n",
    "        user_bow_sum = np.asarray(user_bow.sum(axis=0)).flatten()\n",
    "\n",
    "        top_10_indices_bow = user_bow_sum.argsort()[-10:][::-1]\n",
    "\n",
    "        for index in top_10_indices_bow:\n",
    "            resultados_bow.append({\n",
    "                'id_usuario': usuario,\n",
    "                'palavra': palavras_bow[index],\n",
    "                'contagem': user_bow_sum[index]\n",
    "            })\n",
    "\n",
    "# Converter a lista de resultados em DataFrame\n",
    "resultados_df_bow = pd.DataFrame(resultados_bow)\n",
    "resultados_df_bow.to_csv('dados/com_filtros_datas/6meses/so_suicida_resultados_bow_unigramas.csv', index=False)\n",
    "\n",
    "print(resultados_df_bow)"
   ],
   "id": "2a1dc735f1d6e111",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
