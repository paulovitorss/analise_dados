{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Importando as bibliotecas",
   "id": "2bf9d9363b7b99f2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "from db import connection_db as conndb\n",
    "from db import filters\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "tqdm.pandas()\n",
    "import numpy as np"
   ],
   "id": "4761db26da7a2327",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Conectando ao banco de dados do MongoDB",
   "id": "6ae872a7e38ede3b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "uri = 'mongodb://localhost:27017/'\n",
    "db_name = 'dadosVivamente'\n",
    "col_name = 'dadosSemFiltros'\n",
    "\n",
    "mongo_connection = conndb.MongoDBConnection(uri=uri, database_name=db_name, collection_name=col_name)\n",
    "mongo_connection.connect()\n",
    "collection = mongo_connection.collection"
   ],
   "id": "e2d64bb2f75c6432",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Aplicando pipeline para preparação dos dados",
   "id": "d76def679f1666f8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "collection_filters = filters.CollectionFilters(collection)\n",
    "\n",
    "# Aplicando pipeline 1\n",
    "collection_filters.apply_pipeline1('dadosComFiltrosIniciais')\n",
    "\n",
    "# Aplicando pipeline 2 - pega os posts que tenha a data de publicação de no máximo 7 anos e no mínimo 2 anos\n",
    "collection_filters.apply_pipeline2(7, 1, 'posts7anos1anos')\n",
    "\n",
    "# Aplicando pipeline 3 - desenrola os posts em documentos individuais e cria novas colunas\n",
    "collection_filters.apply_pipeline3('postsComBDIAndInfos')\n",
    "\n",
    "# Aplicando pipeline 4 - aplica o filtro para selecionar apenas os posts que tem uma data de publicação válida\n",
    "collection_filters.apply_pipeline4('postsComBDIAndInfosFiltroDataPosts')\n",
    "\n",
    "collection = collection_filters.collection"
   ],
   "id": "43b4fa30d8a8dfca",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "documentos = collection.find()\n",
    "df_original = pd.DataFrame(list(documentos))"
   ],
   "id": "7801db1d5cf85fe8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Visualizar as primeiras linhas do dataframe\n",
    "df_original.head()"
   ],
   "id": "3a9396b40ef87465",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Visualizar as ultimas linhas do dataframe\n",
    "df_original.tail()"
   ],
   "id": "635b6422d1b17551",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def preencher_e_converter(df, colunas, valor_preenchimento=0, tipo_dados='int64'):\n",
    "    df[colunas] = df[colunas].fillna(valor_preenchimento)\n",
    "    df[colunas] = df[colunas].astype(tipo_dados)\n",
    "    return df\n",
    "\n",
    "\n",
    "# Lista de colunas que precisam ser convertidas\n",
    "colunas_para_converter = [\n",
    "    'pessimismo', 'tristeza', 'fracasso', 'prazer', 'culpa', 'punicao', 'estima',\n",
    "    'critica', 'suicida', 'choro', 'agitacao', 'interesse', 'indecisao',\n",
    "    'desvalorizacao', 'energia', 'sono', 'irritabilidade', 'apetite',\n",
    "    'concentracao', 'fadiga', 'int_sexo', 'quantAmigos'\n",
    "]\n",
    "\n",
    "# Aplicando a função\n",
    "df_original = preencher_e_converter(df_original, colunas_para_converter)\n",
    "\n",
    "# Filtragem dos dados\n",
    "df_original.drop(columns=['_id', 'diaDaSemana', 'hora', 'minutos', 'diaDoMes', 'mes', 'ano'], inplace=True)\n",
    "\n",
    "# Criando novas colunas com o pandas\n",
    "df_original['data'] = df_original['postCreatedTime'].dt.date\n",
    "df_original['data'] = pd.to_datetime(df_original['data'])\n",
    "df_original['mes'] = df_original['data'].dt.to_period('M')\n",
    "df_original['semana'] = df_original['data'].dt.to_period('W')"
   ],
   "id": "6efb32f099f64671",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Filtrar posts do ano de 2016\n",
    "df_2017 = df_original[df_original['data'].dt.year == 2017]\n",
    "df_2017 = df_2017[~((df_2017['data'].dt.month == 12) & (df_2017['data'].dt.year == 2017))]\n",
    "df_2017 = df_2017[df_2017['data'].dt.month >= 5]\n",
    "df_2017 = df_2017[df_2017['suicida'] == 3]\n",
    "df_2017 = df_2017.copy()\n",
    "df_2017.head()"
   ],
   "id": "dd453e5cdcd166dc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Remover linhas onde ambas as colunas 'postMessage' e 'postStory' estão vazias ou nulas\n",
    "df_2017 = df_2017.dropna(subset=['postMessage', 'postStory'], how='all')  # Remove quando ambas são NaN\n",
    "df_2017 = df_2017[~((df_2017['postMessage'].str.strip() == '') & (\n",
    "        df_2017['postStory'].str.strip() == ''))]  # Remove quando ambas são strings vazias\n",
    "\n",
    "df_2017 = df_2017.copy()"
   ],
   "id": "48484e6b079aaa80",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Calcular a contagem de postagens por usuário\n",
    "post_counts = df_2017.groupby('id_usuario').size().reset_index(name='post_count')\n",
    "\n",
    "# Visualizar a distribuição\n",
    "plt.hist(post_counts['post_count'], bins=50)\n",
    "plt.xlabel('Número de Postagens')\n",
    "plt.ylabel('Quantidade de Usuários')\n",
    "plt.title('Distribuição de Postagens por Usuário')\n",
    "plt.show()"
   ],
   "id": "729f238f6359383d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Definir os limites superior e inferior\n",
    "limite_superior = post_counts['post_count'].quantile(0.85)\n",
    "limite_inferior = post_counts['post_count'].quantile(0.15)\n",
    "\n",
    "# Filtrar usuários com alta atividade\n",
    "usuarios_alta_ativ = post_counts[post_counts['post_count'] > limite_superior]['id_usuario']\n",
    "\n",
    "# Filtrar usuários com baixa atividade\n",
    "usuarios_baixa_ativ = post_counts[post_counts['post_count'] < limite_inferior]['id_usuario']\n",
    "\n",
    "# Exibir o número de usuários filtrados\n",
    "print(f'Número de usuários com alta atividade: {len(usuarios_alta_ativ)}')\n",
    "print(f'Número de usuários com baixa atividade: {len(usuarios_baixa_ativ)}')"
   ],
   "id": "1bf3cb162528ff65",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df_normal = df_2017[~(df_2017['id_usuario'].isin(usuarios_baixa_ativ))].copy()\n",
    "norm_post_count = df_normal.groupby('id_usuario').size().reset_index(name='post_count')\n",
    "\n",
    "df_alta_ativ = df_2017[df_2017['id_usuario'].isin(usuarios_alta_ativ)].copy()\n",
    "alt_post_count = df_alta_ativ.groupby('id_usuario').size().reset_index(name='post_count')\n",
    "\n",
    "df_baixa_ativ = df_2017[df_2017['id_usuario'].isin(usuarios_baixa_ativ)].copy()\n",
    "baixa_post_count = df_baixa_ativ.groupby('id_usuario').size().reset_index(name='post_count')"
   ],
   "id": "86f76618acac1043",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def plotar_distribuicao(post_count_df, titulo):\n",
    "    plt.hist(post_count_df['post_count'], bins=50)\n",
    "    plt.xlabel('Número de Postagens')\n",
    "    plt.ylabel('Quantidade de Usuários')\n",
    "    plt.title(titulo)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plotar_distribuicao(norm_post_count, 'Distribuição de Postagens por Usuário (Normal)')\n",
    "plotar_distribuicao(alt_post_count, 'Distribuição de Postagens por Usuário (Alta Atividade)')\n",
    "plotar_distribuicao(baixa_post_count, 'Distribuição de Postagens por Usuário (Baixa Atividade)')"
   ],
   "id": "144c0e2535538ce7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from utils.estracao_interacao import ExtracaoInteracao\n",
    "\n",
    "extracao = ExtracaoInteracao(df_normal)\n",
    "df_with_interactions = extracao.extract_interactions()\n",
    "\n",
    "# Passo 6: Modificar outras colunas usando .loc para evitar avisos\n",
    "df_with_interactions['sexo'] = df_with_interactions['sexo'].map({'F': 0, 'M': 1})\n",
    "df_with_interactions.head()"
   ],
   "id": "b1c9261c30bd2ffb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from utils.text_treatment import TextTreatment\n",
    "from utils.busca_palavras import BuscaPalavras\n",
    "\n",
    "tratamento_texto = TextTreatment()\n",
    "df_with_interactions['postMessageLimpo'] = df_with_interactions['postMessage'].fillna('').progress_apply(\n",
    "    lambda texto: tratamento_texto.preprocessamento_texto(texto) if texto else '')\n",
    "\n",
    "busca_palavras = BuscaPalavras()\n",
    "\n",
    "# Faz a busca exata\n",
    "resultado = busca_palavras.string_matching(df_with_interactions['postMessageLimpo'],\n",
    "                                           'dados/datasets/termos_depressivos_pt_br.txt')\n",
    "\n",
    "# Adicionar a coluna quantPalavrasDepressivas ao DataFrame, contando quantas palavras depressivas foram encontradas\n",
    "df_with_interactions['quantPalavrasDepressivas'] = resultado.apply(lambda x: len(x.split(', ')) if x else 0)\n",
    "df_with_interactions.head()"
   ],
   "id": "512bd3f1dca3c3f6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df_with_interactions.sort_values(['id_usuario', 'data'], inplace=True)\n",
    "\n",
    "# Definindo as variáveis que variam no tempo\n",
    "variaveis_temporais = ['quantProfile', 'quantCover', 'quantAddPhotoWithOthers',\n",
    "                       'quantIsWithOthers', 'quantAddPhoto', 'quantSharedPhoto',\n",
    "                       'quantSharedVideo', 'quantSharedLink', 'quantSharedPost',\n",
    "                       'quantSharedEvent', 'quantSharedMemory', 'quantStatus',\n",
    "                       'quantPalavrasDepressivas']\n",
    "\n",
    "# Primeiro, conte o número de postagens por dia para cada usuário\n",
    "df_postagens_por_dia = df_with_interactions.groupby(['id_usuario', 'data']).size().reset_index(name='num_postagens')\n",
    "\n",
    "# Combine os dados de postagens com as outras variáveis temporais\n",
    "df_grupo = df_with_interactions.groupby(['id_usuario', 'data'])[variaveis_temporais].sum().reset_index()\n",
    "\n",
    "# Mesclar o DataFrame de postagens com as variáveis temporais\n",
    "df_grupo = pd.merge(df_grupo, df_postagens_por_dia, on=['id_usuario', 'data'])\n",
    "\n",
    "janela_temporal = 30\n",
    "\n",
    "\n",
    "def preparar_serie_temporal(df, janela):\n",
    "    series_temporais = []\n",
    "    for usuario in df['id_usuario'].unique():\n",
    "        df_usuario = df[df['id_usuario'] == usuario].set_index('data')\n",
    "\n",
    "        # Aplicando rolling nas variáveis temporais\n",
    "        series_temporais_usuario = df_usuario[variaveis_temporais].rolling(window=janela, min_periods=1).sum()\n",
    "\n",
    "        # Adicionando a coluna 'num_postagens' ao DataFrame\n",
    "        series_temporais_usuario['num_postagens'] = df_usuario['num_postagens']\n",
    "\n",
    "        # Adicionar a coluna 'id_usuario' de volta\n",
    "        series_temporais_usuario['id_usuario'] = usuario\n",
    "\n",
    "        # Adiciona ao resultado final\n",
    "        series_temporais.append(series_temporais_usuario)\n",
    "\n",
    "    return pd.concat(series_temporais)\n",
    "\n",
    "\n",
    "# Aplicando a função para criar janelas temporais deslizantes\n",
    "df_series_temporais = preparar_serie_temporal(df_grupo, janela_temporal).reset_index()"
   ],
   "id": "8622c9b15fde02f2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Preparação do conjunto de treinamento\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "# Preparando a sequência temporal para treinamento\n",
    "for usuario in df_series_temporais['id_usuario'].unique():\n",
    "    df_usuario = df_series_temporais[df_series_temporais['id_usuario'] == usuario]\n",
    "    for i in range(janela_temporal, len(df_usuario)):\n",
    "        # X vai conter as variáveis temporais para a janela atual\n",
    "        X.append(df_usuario.iloc[i - janela_temporal:i][variaveis_temporais].values)\n",
    "\n",
    "        y.append(df_usuario.iloc[i]['num_postagens'])\n",
    "\n",
    "X = np.array(X)  # Transformar em array numpy\n",
    "y = np.array(y)"
   ],
   "id": "399566f61adecb52",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# Dividir os dados em treino e teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Normalizar os dados de entrada (X)\n",
    "scaler_X = MinMaxScaler(feature_range=(0, 1))\n",
    "X_train_scaled = scaler_X.fit_transform(X_train.reshape(-1, X_train.shape[-1])).reshape(X_train.shape)\n",
    "X_test_scaled = scaler_X.transform(X_test.reshape(-1, X_test.shape[-1])).reshape(X_test.shape)\n",
    "\n",
    "# Normalizar os dados de saída (y)\n",
    "scaler_y = MinMaxScaler(feature_range=(0, 1))\n",
    "y_train_scaled = scaler_y.fit_transform(y_train.reshape(-1, 1))\n",
    "y_test_scaled = scaler_y.transform(y_test.reshape(-1, 1))\n",
    "\n",
    "# Criando o modelo LSTM\n",
    "model = Sequential()\n",
    "\n",
    "# Primeira camada LSTM\n",
    "model.add(LSTM(units=50, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# Segunda camada LSTM\n",
    "model.add(LSTM(units=50, return_sequences=False))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# Camada densa final para prever a variável alvo (número de postagens, por exemplo)\n",
    "model.add(Dense(units=1))  # 1 unidade para previsão de uma variável\n",
    "\n",
    "# Compilando o modelo\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Configurando o Early Stopping para interromper o treinamento quando o val_loss não melhorar\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "history = model.fit(X_train_scaled, y_train_scaled, epochs=100, batch_size=32,\n",
    "                    validation_data=(X_test_scaled, y_test_scaled), callbacks=[early_stopping])\n",
    "\n",
    "# Fazer previsões com os dados de teste normalizados\n",
    "previsoes_scaled = model.predict(X_test_scaled)\n",
    "\n",
    "# Desnormalizar as previsões\n",
    "previsoes = scaler_y.inverse_transform(previsoes_scaled)\n",
    "\n",
    "# Comparar previsões com os dados reais (y_test)\n",
    "erro_mse = mean_squared_error(y_test, previsoes)\n",
    "print(f'Erro MSE: {erro_mse}')\n",
    "\n",
    "erro_mae = mean_absolute_error(y_test, previsoes)\n",
    "print(f'Erro MAE: {erro_mae}')"
   ],
   "id": "b020284429f726f3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Plotar as perdas de treino e validação\n",
    "plt.plot(history.history['loss'], label='Loss (Treinamento)')\n",
    "plt.plot(history.history['val_loss'], label='Loss (Validação)')\n",
    "plt.title('Curva de Perda')\n",
    "plt.xlabel('Épocas')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "id": "ce3527595bf88dca",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Plotar os valores reais vs previsões\n",
    "plt.scatter(y_test, previsoes)\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
    "plt.xlabel('Valores Reais')\n",
    "plt.ylabel('Previsões')\n",
    "plt.title('Previsões vs Valores Reais')\n",
    "plt.show()\n"
   ],
   "id": "8a3f0f48b87d08bd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "r2 = r2_score(y_test, previsoes)\n",
    "print(f'R²: {r2}')\n"
   ],
   "id": "b6ba862426544032",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
